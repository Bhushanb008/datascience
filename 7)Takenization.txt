import nltk
import pandas as pd


nltk.download('punkt')


from nltk.tokenize import sent_tokenize
text="""Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue.You shouldn't eat cardboard""";
tokenized_word=sent_tokenize(text)
print(tokenized_word)



from nltk.tokenize import word_tokenize
tokenized_word=word_tokenize(text)
print(tokenized_word)



from nltk.probability import FreqDist
fdist=FreqDist(tokenized_word)
print(fdist)



fdist.most_common(2)


import matplotlib.pyplot as plt 
fdist.plot(38, cumulative=False)
plt.show()




nltk.download ('stopwords')
from nltk.corpus import stopwords
stop_words=set (stopwords.words ("english"))
print (stop_words)




filtered_sent=[]
for w in tokenized_word: 
  if w not in stop_words:
    filtered_sent.append (w)
print("tokenized sentence: ", tokenized_word) 
print("filtered sentence: ", filtered_sent)



from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
ps=PorterStemmer()
stemmed_words=[]
for w in filtered_sent:
  stemmed_words.append(ps.stem(w))
print("stemmed sentence: ", stemmed_words)
print("filtered sentence: ",filtered_sent)




nltk.download('wordnet')
from nltk.stem.wordnet import WordNetLemmatizer
lem=WordNetLemmatizer()
from nltk.stem.porter import PorterStemmer
stem=PorterStemmer()
word="flying"
print("Lemmatized word: ",lem.lemmatize(word))
print("stemed word: ", stem.stem(word))


sent="Albert Einstein was born in ulm, Germany in 1879."



tokens=nltk.word_tokenize(sent) 
print(tokens)



nltk.download('averaged_perceptron_tagger')
nltk.pos_tag(tokens)
